---
title: "Where are the Starbuck's At?"
author: "Will Jones"
date: "May 11, 2015"
output:
  html_document:
    keep_md: yes
    toc: yes
---

```{r, echo=FALSE, warning=FALSE}
# Load all the packages
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggmap))
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(rgdal))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(jsonlite))
suppressPackageStartupMessages(library(RCurl))
suppressPackageStartupMessages(library(rgeos))



# Set up parallel processing
cpuCount <- detectCores() #From the parallel package
registerDoParallel(cores=cpuCount-1) #Don't take all cores.
```

This project examines correlations between the number of Starbuck's locations
in an area with the rate of poverty in an area. For the locations, this project
examines census tracts in Los Angeles County.

## Getting the Data

Census data is provided by the 2010 census for estimates of population and
ethnic makeup, while we use ACS five-year estimates for median income and
poverty level.

```{r, echo=FALSE}
# Get 2010 census data
la.census <- read.csv('census_data/LA_census_2010.csv', 
                      colClasses=c(rep("character", 10), 
                                   rep("numeric", 62))) %>% 
                        tbl_df()
la.census %<>% 
  # select only Los Angeles County
  filter(Geo_COUNTY == '037') %>%
  rename(population = SE_T001_001,
         white = SE_T054_002)

# Get income census data (from 5-year estimates)
la.income <- read.csv('census_data/LA_income.csv',
                      colClasses=c(rep('character', 5),
                                   rep('factor', 15),
                                   rep('numeric', 82))) %>%
  tbl_df() %>%
  filter(Geo_COUNTY == '037') %>%
  rename(median.income = SE_T057_001,
         pop.poverty = SE_T118_002)

# Combine the two data frames
la.census %<>% left_join(la.income, by='Geo_FIPS') %>%
  select(Geo_FIPS, population, white, median.income, pop.poverty)

la.census$Geo_FIPS %<>% as.character()

# Convert white and poverty to rates in percents
la.census %<>%
  mutate(white = white / population * 100,
         rate.poverty = pop.poverty / population * 100)
```

Here are some histograms of the basic demographic data:

```{r, echo=FALSE}
par(mfrow=c(2,2))
hist(la.census$white, main="White % Population")
hist(la.census$rate.poverty, main="% in Poverty")
hist(la.census$median.income, main="Median Income")
hist(la.census$population, main="Population")
```





```{r, echo=FALSE, warning=FALSE}
# Number of census tracts to sample
n <- 2
search.text <- 'Starbucks Coffee'

# Loading the shape file
# LA.shapefile <- readOGR(dsn="los_angeles/", 
#                         layer="eGIS_Demographic.EGIS", 
#                        verbose=FALSE) %>%
#  spTransform(CRS("+proj=longlat +ellps=WGS84"))

# LA.data <- LA.shapefile@data %>% tbl_df()

# I wasn't able to get this to work on my R installation, but Albert Kim was
# able to do so and save it as a data frame.
# LA.map <- fortify(LA.shapefile, region='GEOID10') %>% tbl_df()
load('cached_data/LA.RData') # saved as LA.map

# Subset shapefile to tracts we want to look at
LA.map %<>% 
  inner_join(la.census, by=c('id' = 'Geo_FIPS')) %>%
  filter(lat > 33.5) # Get rid of weird census tracts to the south

the.FIPS <- unique(LA.map$id)
# Make sure we have an equal number of census tracts between the two
la.census %<>% filter(Geo_FIPS %in% the.FIPS)
```

Now that we have a selection of census tracts to examine, we need to find how
many Starbucks locations are nearby each. To get all the relevant locations in
the area, we can do a series of searches on Google around specially chosen
points. We can do this with 24 radar searches with radii of 13 kilometers. The
maximum radius is 50km, but this increases the likelihood that we will miss
locations, for the API call only returns up to 200 locations at once.(The
function here to draw circles was adapted from code written by
Gregoire Vincke, in a 
[Stack Overflow answer](http://stackoverflow.com/a/29133886/4645559).)
```{r, echo=FALSE}
makeCircle <- function(LonDec, LatDec, Km, group) {#Corrected function
    #LatDec = latitude in decimal degrees of the center of the circle
    #LonDec = longitude in decimal degrees
    #Km = radius of the circle in kilometers
    ER <- 6371 #Mean Earth radius in kilometers. Change this to 3959 and you will have your function working in miles.
    AngDeg <- seq(1:360) #angles in degrees 
    Lat1Rad <- LatDec*(pi/180)#Latitude of the center of the circle in radians
    Lon1Rad <- LonDec*(pi/180)#Longitude of the center of the circle in radians
    AngRad <- AngDeg*(pi/180)#angles in radians
    Lat2Rad <-asin(sin(Lat1Rad)*cos(Km/ER)+cos(Lat1Rad)*sin(Km/ER)*cos(AngRad)) #Latitude of each point of the circle rearding to angle in radians
    Lon2Rad <- Lon1Rad+atan2(sin(AngRad)*sin(Km/ER)*cos(Lat1Rad),cos(Km/ER)-sin(Lat1Rad)*sin(Lat2Rad))#Longitude of each point of the circle rearding to angle in radians
    Lat2Deg <- Lat2Rad*(180/pi)#Latitude of each point of the circle rearding to angle in degrees (conversion of radians to degrees deg = rad*(180/pi) )
    Lon2Deg <- Lon2Rad*(180/pi)#Longitude of each point of the circle rearding to angle in degrees (conversion of radians to degrees deg = rad*(180/pi) )
    # output dataframe of locations
    return(data_frame(long = Lon2Deg, lat = Lat2Deg, group = group))
}

LA.plot <- ggplot() +
  geom_polygon(aes(x = long, y = lat, group=group), data=LA.map)

search.centers <- data_frame(
  long = c(seq(-118.6, -117.75, length.out=6),
           seq(-118.6, -117.74, length.out=5),
           seq(-118.85, -117.75, length.out=7),
           seq(-118.4, -117.85, length.out = 4),
           seq(-118.31, -118.13, length.out = 2)
           ),
  lat = c(rep(34.41, 6),
          rep(34.24, 5),
          rep(34.085, 7),
          rep(33.95, 4),
          rep(33.78, 2)))

circles <- data_frame(long = numeric(), lat = numeric(),
           group = integer())

for (i in 1:nrow(search.centers)) {
  long <- search.centers$long[i]
  lat <- search.centers$lat[i]
  radius <- 13 #km
  circles %<>% bind_rows(makeCircle(long, lat, radius, i))
}

LA.plot + 
  geom_point(aes(x = long, y = lat), data=search.centers,
             color = 'red')  + 
  geom_polygon(aes(x = long, y = lat, group=group), data=circles,
               color='red', fill = 'red', linetype='dotted',
               alpha=0.2) + 
  labs(title = "Radar Search Locations") + 
  theme_bw() + coord_map()


```



```{r, echo=FALSE}
# Define function to get nearby locations
getLocations <- function(x, y, query, radius=10000) {
  # Uses Google Places to find all locations near a place.
  # Remember we want to over shoot, and then count those that
  # are actually near the borders of the 
  #
  # Args: 
  #  x: the lattitude of the center from which to search
  #  y: the longitude of the center from which to search
  #  radius: the radius of the search, in meters
  #  query: the name of the franchise to search for
  #
  # Returns:
  #  A dataframe of locations.
  
  # First, construct the query URL
  gplaces.key <- readLines('Google_API_key.txt')
  url <- 'https://maps.googleapis.com/maps/api/place/radarsearch/json?'
  url %<>% paste('key=', gplaces.key, '&', sep='')
  url %<>% paste('location=', as.character(x), ',',
                 as.character(y), '&', sep='')
  url %<>% paste('radius=', as.character(radius), '&', sep='')
  query %<>% str_replace(' ', '+')
  url %<>% paste('keyword=', query, sep='')
  
  data <- url %>% getURL() %>% fromJSON()
  
  if (data$status != "ZERO_RESULTS") {
    
    data %<>% .$results %>% as.data.frame()
    
    output <- data_frame(place_id = data$place_id,
                         lat = data$geometry$location$lat,
                         long = data$geometry$location$lng)
    return(output) 
    } else {
      return(data_frame(place_id = character(),
                        lat = numeric(),
                        long = numeric()))
      }
  }

# test <- getLocations(34.0429419, -118.2657636, 'starbucks coffee')

search.text <- "Starbucks Coffee"

#locations <- data_frame(place_id = character(),
#                        lat = numeric(),
#                        long = numeric())

#for (i in 1:nrow(search.centers)) {
#  long <- search.centers$long[i]
#  lat <- search.centers$lat[i]
#  
#  new.locations <- getLocations(lat,
#                                long,
#                                search.text)
#  locations %<>% union(new.locations)
#}
  
# Saved Locations
#save(locations, file='sblocations.RData')
load('cached_data/sblocations.RData')

ggplot() +
  geom_polygon(data = LA.map,
               aes(x=long, y=lat, group=group)) +
  geom_point(data = locations,
             aes(x = long, y = lat),
             color = 'green') + 
  coord_map() +
  theme_bw() + 
  labs(title = "Locations of Starbucks in Los Angeles County")

```

Notice there are some areas with no locations. To the north is Angeles National
Forest, and the western stretch is Santa Monica Mountains Recreational Area. In
some areas, that are likely less populated, the locations seem to align with
whatever freeways are nearby.

Now we need to calculate how many Starbuck's locations are near each census
tract.

```{r, echo=FALSE}
countLocations <- function(polygon, locations, radius=2) {
  # Counts the number of locations that are either inside the polygon, or are 
  # within the given radius of a border.
  #
  # Args:
  #  polygon: a dataframe of edges of the polygon
  #  locations: a dataframe of the locations, which is returned by getLocations
  #  radius: a radius in km to be the max valide distance from accessible location
  #
  # Returns: 
  #  An integer indicating how many locations are near the polygon
  
  # First, check if the point is actually inside the polygon
  output <- point.in.polygon(locations$long, locations$lat, 
                             polygon$long, polygon$lat)
  
  # If that doesn't work, check whether they are within a certain radius of the
  # edges of of polygon.
  
  # Convert radius to long and lat radius
  earth.radius <- 6371 # km
  deg.to.rad <- pi/180
  rad.to.deg <- 180/pi
  approx.lat <- mean(polygon$lat) # lat at census tract
  radius.lat <- ( radius/ earth.radius) * rad.to.deg
  radius.at.lat <- earth.radius * cos(approx.lat * deg.to.rad)
  radius.long <- (radius / radius.at.lat) * rad.to.deg
  
  for (i in 1:nrow(locations)) {
    if (output[i] == FALSE) {
      location <- locations[i,]
      long <- location$long
      lat <- location$lat
      
      # Check if there is even a chance of intersection
      polygon.long.range <- range(polygon$long) + c(-1,1) * radius.long
      polygon.lat.range <- range(polygon$lat) + c(-1, 1) * radius.lat
      in.int <- function(x,y) y[1] <= x & x <= y[2]
      nearby <- (in.int(long, polygon.long.range) &
                   in.int(lat, polygon.lat.range))
      
      if (nearby) {
        # Then make a circle and check all the points
        circle.points <- makeCircle(long, lat, radius, 1)
        # Now we check if the circle and polygon over lap. Looks overly
        # complicated? It is. Thanks a lot sp package...
        circle <- cbind(circle.points$long, circle.points$lat) %>%
          Polygon() %>%
          list() %>% Polygons(1) %>%
          list() %>% SpatialPolygons()
        tract <- cbind(polygon$long, polygon$lat) %>%
          Polygon() %>%
          list() %>% Polygons(2) %>%
          list() %>% SpatialPolygons()
        overlaps <- gOverlaps(circle, tract)
        contains <- gContains(circle, tract) | gContains(tract, circle)
        output[i] <- overlaps | contains
        #print(gOverlaps(circle, tract))
        }
      
      }
    }
  
  output %>% sum() %>% return()
  }
  

la.census$sb.count = numeric(length = nrow(la.census))  


# Single thread version
#  for (i in 1:(nrow(la.census)) ) {
#    row <- la.census[i,]
#    FIPS <- row$Geo_FIPS
#    polygon <- LA.map %>% filter(id == FIPS)
#    la.census[i,]$sb.count <- countLocations(polygon, locations)
#  }

getLocationCount <- function (row.iter) {
  obs <- la.census[row.iter,]
  FIPS <- obs$Geo_FIPS
  polygon <- LA.map %>% filter(id == FIPS)
  countLocations(polygon, locations, radius=2)
  }

#loc.count.result <- foreach(i=1:nrow(la.census), .combine=c) %dopar%
#  getLocationCount(i)


#la.census$sb.count <- loc.count.result

#save(la.census, file='census_with_counts.RData')
load('cached_data/census_with_counts.RData')

qplot(x = la.census$sb.count, main="Histogram of # of Starbuck's") + 
  theme_bw()
```

## Exploring the Data

Looking at the population histogram again, there are several census tracts with
very few, people that give outrageously high number of Starbuck's per 1000
people. Thus, it may make sense to only look at census tracts with more than
2000 people in them. (The Census Bureau usually aims to keep census tracts near
4000 population.)
```{r, echo=FALSE}
la.census %<>% mutate(sb.per.1000 = sb.count * 1000 / population,
                      sb.per.1000 = ifelse(sb.per.1000 == Inf, NA, sb.per.1000))

qplot(x = population, data = la.census) + theme_bw()
la.census %>% arrange(desc(sb.per.1000))
# Filter out the low population tracts
la.census %<>% mutate(sb.per.1000 = ifelse(population < 1000, NA, sb.per.1000))
qplot(x = population, y = sb.per.1000, data = la.census) + theme_bw() + 
  labs(title = "Population and SB Rate After Chucking Small Pops",
       x = "Population", y = "Starbuck's Count per 1000")
```

Notice there still is a relationship here between population and Starbuck's per
1000 people. 


```{r, echo=FALSE}

LA.map <- la.census %>%
  select(Geo_FIPS, sb.count, sb.per.1000) %>%
  right_join(LA.map, by=c('Geo_FIPS' = 'id'))


ggplot() +
  geom_polygon(data = LA.map,
               aes(x=long, y=lat, group=group, fill=population)) + 
  labs(title = "Population") + theme_bw() + coord_map()

ggplot() + geom_polygon(data = LA.map,
                        aes(x = long, y = lat, group = group,
                            fill = rate.poverty)) + 
  labs(title = "Poverty Rate") + theme_bw()+ coord_map()

ggplot() + geom_polygon(data = LA.map,
                        aes(x = long, y = lat, group = group,
                            fill = white)) + 
  labs(title = "% White") + theme_bw()+ coord_map()

ggplot() + geom_polygon(data = LA.map,
                        aes(x = long, y = lat, group = group,
                            fill = sb.count)) + 
  labs(title = "Starbuck's Count") + theme_bw()+ coord_map()

ggplot() + geom_polygon(data = LA.map,
                        aes(x = long, y = lat, group = group,
                            fill = sb.per.1000)) + 
  labs(title = "Starbuck's Per 1000 Population") + theme_bw()+ coord_map()
```

There is one census tract that has 25 Starbuck's near it, but none actaully in
it. This is Griffith Park.





## Fitting a Model
Now we need to choose a model for this data. We could try a Poisson model, as
our response variable is a count, but the variance of the data is greater than
the mean, and thus our data is "overdispersed."
```{r}
mean(la.census$sb.count)
var(la.census$sb.count)
```

To correct for this, we will model the data with a quasi-Poisson family. We are
filtering out the tracts with low population, and then offsetting by population,
so that it is taken into account in the model. 
```{r}
model <- la.census %>% filter(population > 1000) %>%
  glm(sb.count ~ 1, data = ., offset=log(population), 
      family=quasipoisson)
summary(model)
# The average number of Starbucks per 1000 people
exp(model$coefficients)*1000

qplot(x = model$residuals)
```

### Starbuck's Counts and Poverty

Now here is the question: Are the distributions of Starbuck's counts, taking
into account population, statistcally different depending on the level of
poverty? 

```{r}
la.census %<>% mutate(poverty = rate.poverty > 40)

modelA <- la.census %>% filter(population > 1000 & poverty) %>%
  glm(sb.count ~ 1, data = ., offset=log(population), 
      family=quasipoisson)
# The average number of Starbucks per 1000 people
exp(modelA$coefficients)*1000


modelB <- la.census %>% filter(population > 1000 & !poverty) %>%
  glm(sb.count ~ 1, data = ., offset=log(population), 
      family=quasipoisson)
# The average number of Starbucks per 1000 people
exp(modelB$coefficients)*1000
```

If we split up the census tracts by whether their povery rate is greater than
40%, we find that the intercept coefficient is fairly different, suggesting there
are different rates in the two groups.

We could try a doing a model with `glm` of `sb.count ~ rate.poverty`, but
our Starbuck's location counts are __not independent__ at all; the number of
Starbuck's near one census tract is highly correlated with the count for a
neighboring census tract.

Instead, we can use a permutation test. Let $\lambda_A$ be the intercept coefficient
for the census tracts that have a poverty rate greater than 40% and $\lambda_B$ be
the intercept coefficient for the rest of the census tracts.

* $H_0: \lambda_A - \lambda_B = 0$
* $H_A: \lambda_A - \lambda_B \neq 0$

```{r}
T_obs <- exp(modelA$coefficients)*1000 - exp(modelB$coefficients)*1000
T_obs
```



```{r, echo=FALSE}
count <- la.census %>% filter(poverty) %>% count() %>% .$n
iterations <- 1000
  
# Create a column of shuffle-able poverty bool values
LA.shuffle <- la.census %>%
  filter(population > 1000) %>%
  select(poverty, population, sb.count)

calcCoef <- function (df) {
  # Computes the average number of Starbucks per 1000 people
  model <- glm(sb.count ~ 1, data = df, offset = log(population),
               family = quasipoisson)
  return(exp(model$coefficients)*1000)
}

sampleT <- function (n) {
  shuffle <- LA.shuffle %>% nrow() %>% sample()
  LA.shuffle$poverty <- LA.shuffle$poverty[shuffle]
  
  lambdaA <- LA.shuffle %>% filter(poverty) %>% calcCoef()
  lambdaB <- LA.shuffle %>% filter(!poverty) %>% calcCoef()
  return(lambdaA - lambdaB)
}

Tvalues <- numeric(length=iterations)

for (i in 1:iterations) {
  Tvalues[i] <- sampleT(count)
}

qplot(x = Tvalues) + 
  geom_vline(xintercept=T_obs, color = 'red') + 
  theme_bw() + 
  labs(title = "Null Distribution of Poverty - Non-Poverty")

```
The probability of getting a value of $T$ greater than our $T_{obs}$ from the
null distribution is 
```{r}
(sum(Tvalues >= T_obs) + 1)/(iterations+1)
```

Thus, it is safe to say that there is a statistically significant difference
between the distribution of Starbuck's counts in poorer and wealthier areas.

We can construct confidence intervals for these values using a similar method
called Bootstrapping.

```{r, echo=FALSE}
bootstrap.CI <- function(iterations) {
  # Place to store the bootstrapped samples
  boot.sample.T <- numeric(length = iterations)
  boot.sample.F <- numeric(length = iterations)
  # New subsetted data to sample from
  data.T <- la.census %>% filter(poverty & population > 1000)
  data.F <- la.census %>% filter(!poverty & population > 1000)
  # Perform first bootstrap
  for (i in 1:iterations) {
    boot.sample.T[i] <- data.T %>% 
      sample_n(nrow(data.T), replace=TRUE) %>% 
      calcCoef()
  }
  # Perform second bootstrap
  for (i in 1:iterations) {
    boot.sample.F[i] <- data.F %>% 
      sample_n(nrow(data.F), replace=TRUE) %>%
      calcCoef()
  }
  alpha <- 0.05
  CI <- c(alpha/2, 1 - alpha/2)
  list(quantile(boot.sample.T, CI),
       quantile(boot.sample.F, CI)) %>% return()
}
```

```{r, warning=FALSE}
bootstrap.CI(1000)
```


### Other Variables
We could do similar analyses with variables like median income and percentage of
population that is white. Both of these are strongly correlated with poverty
level in the Los Angeles area, as can be seen below.
```{r, echo=FALSE,warning=FALSE}
qplot(x = rate.poverty, y = white, data = la.census)
qplot(x = rate.poverty, y = log(median.income), data = la.census)
qplot(x = white, y = log(median.income), data = la.census)
```
Thus, analyses of these will likely not provide much more information.







