---
title: "Where are the Starbuck's At?"
author: "Will Jones"
date: "May 11, 2015"
output:
  html_document:
    keep_md: yes
    toc: yes
---

```{r, echo=FALSE, warning=FALSE}
# Load all the packages
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(ggmap))
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(rgdal))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(jsonlite))
suppressPackageStartupMessages(library(RCurl))
suppressPackageStartupMessages(library(rgeos))



# Set up parallel processing
cpuCount <- detectCores() #From the parallel package
registerDoParallel(cores=cpuCount-1) #Don't take all cores.
```

This project examines correlations between the number of Starbuck's locations
in an area with the rate of poverty in an area. For the locations, this project
examines census tracts in Los Angeles County.

More generally, this project explored scraping locations from Google's Places
API to determine the number of locations near census tracts. This project also
discusses analyizing this type of data, which has really strong spatial
correlation between data points. In analyizing the data, I used a permuatation
test with Monte Carlo sampling and bootstrapping to test the significance of
differences in rates of Starbuck's per 1,000 population in low and high poverty
areas and get confidence intervals for the average rates.

I found that census tracts in which there is higher levels of poverty (more than
40% of the population below the poverty line), there are more Starbuck's
locations per 1,000 people. In low-poverty tracts, the rate was 1.02 
(95% CI [0.98, 1.05]), whereas for high-poverty tracts, the rate was 1.28
(95% CI [0.97, 1.62]).

This anaylsis could easily be adapted to other scenarios (that are probably more
interesting, to be honest.) For example, how are medical marijuana clinics or
payday loan businesses correlated with povery in surrounding areas? How does
this vary between different cities? 

## Getting the Data
Census data and shapefiles can be downloaded, but the location data for the
Starbuck's location needs to be scraped from Google. 

### Census Data

Census data is provided by the 2010 census for estimates of population and
ethnic makeup, while we use American Community Survey five-year estimates for 
median income and poverty level. The csv files were compiled and downloaded on
[Social Explorer](http://www.socialexplorer.com).

```{r, echo=FALSE}
# Get 2010 census data
la.census <- read.csv('census_data/LA_census_2010.csv', 
                      colClasses=c(rep("character", 10), 
                                   rep("numeric", 62))) %>% 
                        tbl_df()
la.census %<>% 
  # select only Los Angeles County
  filter(Geo_COUNTY == '037') %>%
  rename(population = SE_T001_001)

# Get income census data (from 5-year estimates)
la.income <- read.csv('census_data/LA_income.csv',
                      colClasses=c(rep('character', 5),
                                   rep('factor', 15),
                                   rep('numeric', 82))) %>%
  tbl_df() %>%
  filter(Geo_COUNTY == '037') %>%
  rename(pop.poverty = SE_T118_002)

# Combine the two data frames
la.census %<>% left_join(la.income, by='Geo_FIPS') %>%
  select(Geo_FIPS, population, pop.poverty)

la.census$Geo_FIPS %<>% as.character()

# Convert white and poverty to rates in percents
la.census %<>%
  mutate(rate.poverty = pop.poverty / population * 100)
```

Here are some histograms of the basic demographic data:

```{r, echo=FALSE}
par(mfrow=c(1,2))
hist(la.census$rate.poverty, main="% in Poverty")
hist(la.census$population, main="Population")
```

### Los Angeles County Shapefiles
The shapefiles were acquired from the [County of Los Angeles' Open Data Portal](https://data.lacounty.gov/Geospatial/Census-Tracts-2010/ay2y-b9rg). 

```{r, echo=FALSE, warning=FALSE}

# Loading the shape file
# LA.shapefile <- readOGR(dsn="los_angeles/", 
#                         layer="eGIS_Demographic.EGIS", 
#                        verbose=FALSE) %>%
#  spTransform(CRS("+proj=longlat +ellps=WGS84"))

# LA.data <- LA.shapefile@data %>% tbl_df()

# I wasn't able to get this to work on my R installation, but Albert Kim was
# able to do so and save it as a data frame.
# LA.map <- fortify(LA.shapefile, region='GEOID10') %>% tbl_df()
load('cached_data/LA.RData') # saved as LA.map

# Subset shapefile to tracts we want to look at
LA.map %<>% 
  inner_join(la.census, by=c('id' = 'Geo_FIPS')) %>%
  filter(lat > 33.5) # Get rid of weird census tracts to the south

the.FIPS <- unique(LA.map$id)
# Make sure we have an equal number of census tracts between the two
la.census %<>% filter(Geo_FIPS %in% the.FIPS)
```

### Scrapping the Location Data

Now that we have a selection of census tracts to examine, we need to find how
many Starbucks locations are nearby each. To get all the relevant locations in
the area, we can do a series of searches on Google around specially chosen
points. 

[Google's Places API](https://developers.google.com/places/webservice/) allows
developers to send HTTP request for "Radar Searches," which take a lat/long
location, a radius and search query and returns up to 200 locations in the
given radius that match the query. 

Thus, if we can cover all of LA County with a series of Radar Searches, we can
find all the locations of all Starbuck's in the area. It turns out we can
accomplish this with just 24 radar searches, each with radii of 13 kilometers.
(The maximum radius is 50km, but this increases the likelihood that we will miss
locations, for the API call only returns up to 200 locations at once.)

Below is a plot showing the arrangement of all the radar searches. (The function
used here to draw circles was adapted from code written by Gregoire Vincke, in a 
[Stack Overflow answer](http://stackoverflow.com/a/29133886/4645559).)

```{r, echo=FALSE, fig.width=4}
makeCircle <- function(LonDec, LatDec, Km, group) {#Corrected function
    #LatDec = latitude in decimal degrees of the center of the circle
    #LonDec = longitude in decimal degrees
    #Km = radius of the circle in kilometers
    ER <- 6371 #Mean Earth radius in kilometers. Change this to 3959 and you will have your function working in miles.
    AngDeg <- seq(1:360) #angles in degrees 
    Lat1Rad <- LatDec*(pi/180)#Latitude of the center of the circle in radians
    Lon1Rad <- LonDec*(pi/180)#Longitude of the center of the circle in radians
    AngRad <- AngDeg*(pi/180)#angles in radians
    Lat2Rad <-asin(sin(Lat1Rad)*cos(Km/ER)+cos(Lat1Rad)*sin(Km/ER)*cos(AngRad)) #Latitude of each point of the circle rearding to angle in radians
    Lon2Rad <- Lon1Rad+atan2(sin(AngRad)*sin(Km/ER)*cos(Lat1Rad),cos(Km/ER)-sin(Lat1Rad)*sin(Lat2Rad))#Longitude of each point of the circle rearding to angle in radians
    Lat2Deg <- Lat2Rad*(180/pi)#Latitude of each point of the circle rearding to angle in degrees (conversion of radians to degrees deg = rad*(180/pi) )
    Lon2Deg <- Lon2Rad*(180/pi)#Longitude of each point of the circle rearding to angle in degrees (conversion of radians to degrees deg = rad*(180/pi) )
    # output dataframe of locations
    return(data_frame(long = Lon2Deg, lat = Lat2Deg, group = group))
}

LA.plot <- ggplot() +
  geom_polygon(aes(x = long, y = lat, group=group), data=LA.map)

search.centers <- data_frame(
  long = c(seq(-118.6, -117.75, length.out=6),
           seq(-118.6, -117.74, length.out=5),
           seq(-118.85, -117.75, length.out=7),
           seq(-118.4, -117.85, length.out = 4),
           seq(-118.31, -118.13, length.out = 2)
           ),
  lat = c(rep(34.41, 6),
          rep(34.24, 5),
          rep(34.085, 7),
          rep(33.95, 4),
          rep(33.78, 2)))

circles <- data_frame(long = numeric(), lat = numeric(),
           group = integer())

for (i in 1:nrow(search.centers)) {
  long <- search.centers$long[i]
  lat <- search.centers$lat[i]
  radius <- 13 #km
  circles %<>% bind_rows(makeCircle(long, lat, radius, i))
}

LA.plot + 
  geom_point(aes(x = long, y = lat), data=search.centers,
             color = 'red')  + 
  geom_polygon(aes(x = long, y = lat, group=group), data=circles,
               color='red', fill = 'red', linetype='dotted',
               alpha=0.2) + 
  labs(title = "Radar Search Locations") + 
  theme_bw() + coord_map()


```

The locations have been saved as a file and are just loaded from that file in
this document. For those who are interested, though, the code used to scrape the
location data is present in the RMD file, with the portions that actually
execute the search commented out. (In order to make use of the code though, you
will need to get your own API key from Google.)

```{r, echo=FALSE}
# Define function to get nearby locations
getLocations <- function(x, y, query, radius=10000) {
  # Uses Google Places to find all locations near a place.
  # Remember we want to over shoot, and then count those that
  # are actually near the borders of the 
  #
  # Args: 
  #  x: the lattitude of the center from which to search
  #  y: the longitude of the center from which to search
  #  radius: the radius of the search, in meters
  #  query: the name of the franchise to search for
  #
  # Returns:
  #  A dataframe of locations.
  
  # First, construct the query URL
  gplaces.key <- readLines('Google_API_key.txt')
  url <- 'https://maps.googleapis.com/maps/api/place/radarsearch/json?'
  url %<>% paste('key=', gplaces.key, '&', sep='')
  url %<>% paste('location=', as.character(x), ',',
                 as.character(y), '&', sep='')
  url %<>% paste('radius=', as.character(radius), '&', sep='')
  query %<>% str_replace(' ', '+')
  url %<>% paste('keyword=', query, sep='')
  
  data <- url %>% getURL() %>% fromJSON()
  
  if (data$status != "ZERO_RESULTS") {
    
    data %<>% .$results %>% as.data.frame()
    
    output <- data_frame(place_id = data$place_id,
                         lat = data$geometry$location$lat,
                         long = data$geometry$location$lng)
    return(output) 
    } else {
      return(data_frame(place_id = character(),
                        lat = numeric(),
                        long = numeric()))
      }
  }

# test <- getLocations(34.0429419, -118.2657636, 'starbucks coffee')

search.text <- "Starbucks Coffee"

#locations <- data_frame(place_id = character(),
#                        lat = numeric(),
#                        long = numeric())

#for (i in 1:nrow(search.centers)) {
#  long <- search.centers$long[i]
#  lat <- search.centers$lat[i]
#  
#  new.locations <- getLocations(lat,
#                                long,
#                                search.text)
#  locations %<>% union(new.locations)
#}
  
# Saved Locations
#save(locations, file='sblocations.RData')
load('cached_data/sblocations.RData')

ggplot() +
  geom_polygon(data = LA.map,
               aes(x=long, y=lat, group=group)) +
  geom_point(data = locations,
             aes(x = long, y = lat),
             color = 'green') + 
  coord_map() +
  theme_bw() + 
  labs(title = "Locations of Starbucks in Los Angeles County")

```

Notice there are some areas with no locations. To the north is Angeles National
Forest, and the western stretch is Santa Monica Mountains Recreational Area. In
some areas, that are likely less populated, the locations seem to align with
whatever freeways are nearby.

From these locations we can calculate the number of Starbuck's nearby for each
census tract. We will consider a Starbuck's location nearby if it is within
2 kilometers of the border of the census tract.

The results of this data manipulation are also saved as an R data dump, because
of how long they took to calculate. Once again, the code to make the
calculations can be found in the RMD file, with the lines that execute the
calculate commented out. (Please take a moment of silence to mourn the many
hours I lost to figuring out how to draw a circle on a spherical Earth and 
then detect whether that circle intersected with a polygon that also exists on a
spherical Earth.)

```{r, echo=FALSE, fig.width=4}
countLocations <- function(polygon, locations, radius=2) {
  # Counts the number of locations that are either inside the polygon, or are 
  # within the given radius of a border.
  #
  # Args:
  #  polygon: a dataframe of edges of the polygon
  #  locations: a dataframe of the locations, which is returned by getLocations
  #  radius: a radius in km to be the max valide distance from accessible location
  #
  # Returns: 
  #  An integer indicating how many locations are near the polygon
  
  # First, check if the point is actually inside the polygon
  output <- point.in.polygon(locations$long, locations$lat, 
                             polygon$long, polygon$lat)
  
  # If that doesn't work, check whether they are within a certain radius of the
  # edges of of polygon.
  
  # Convert radius to long and lat radius
  earth.radius <- 6371 # km
  deg.to.rad <- pi/180
  rad.to.deg <- 180/pi
  approx.lat <- mean(polygon$lat) # lat at census tract
  radius.lat <- ( radius/ earth.radius) * rad.to.deg
  radius.at.lat <- earth.radius * cos(approx.lat * deg.to.rad)
  radius.long <- (radius / radius.at.lat) * rad.to.deg
  
  for (i in 1:nrow(locations)) {
    if (output[i] == FALSE) {
      location <- locations[i,]
      long <- location$long
      lat <- location$lat
      
      # Check if there is even a chance of intersection
      polygon.long.range <- range(polygon$long) + c(-1,1) * radius.long
      polygon.lat.range <- range(polygon$lat) + c(-1, 1) * radius.lat
      in.int <- function(x,y) y[1] <= x & x <= y[2]
      nearby <- (in.int(long, polygon.long.range) &
                   in.int(lat, polygon.lat.range))
      
      if (nearby) {
        # Then make a circle and check all the points
        circle.points <- makeCircle(long, lat, radius, 1)
        # Now we check if the circle and polygon over lap. Looks overly
        # complicated? It is. Thanks a lot sp package...
        circle <- cbind(circle.points$long, circle.points$lat) %>%
          Polygon() %>%
          list() %>% Polygons(1) %>%
          list() %>% SpatialPolygons()
        tract <- cbind(polygon$long, polygon$lat) %>%
          Polygon() %>%
          list() %>% Polygons(2) %>%
          list() %>% SpatialPolygons()
        overlaps <- gOverlaps(circle, tract)
        contains <- gContains(circle, tract) | gContains(tract, circle)
        output[i] <- overlaps | contains
        #print(gOverlaps(circle, tract))
        }
      
      }
    }
  
  output %>% sum() %>% return()
  }
  

la.census$sb.count = numeric(length = nrow(la.census))  


getLocationCount <- function (row.iter) {
  obs <- la.census[row.iter,]
  FIPS <- obs$Geo_FIPS
  polygon <- LA.map %>% filter(id == FIPS)
  countLocations(polygon, locations, radius=2)
  }

#loc.count.result <- foreach(i=1:nrow(la.census), .combine=c) %dopar%
#  getLocationCount(i)


#la.census$sb.count <- loc.count.result

#save(la.census, file='census_with_counts.RData')
load('cached_data/census_with_counts.RData')

qplot(x = la.census$sb.count, main="Histogram of # of Starbuck's",
      binwidth = 1) + 
  theme_bw()
```



## Exploring the Data

Looking at the population histogram again, there are several census tracts with
very few, people that give outrageously high number of Starbuck's per 1000
people. If we look at the map with population and the locations, it becomes
clear why. Census tracts that are mainly parks have very small populations, but
are so large that they border a lot of census tracts that are populated, and
thus pick up the Starbuck's locations from those. Thus, it may make sense to
only look at census tracts with more than 2000 people in them. (The Census 
Bureau usually aims to keep census tracts near 4000 population.)

```{r, echo=FALSE, fig.width=4}
ggplot() +
  geom_polygon(data = LA.map,
               aes(x=long, y=lat, group=group, fill=population)) +
  geom_point(data = locations,
             aes(x = long, y = lat),
             color = 'green') + 
  coord_map() +
  theme_bw() + 
  labs(title = "Locations of Starbucks in Los Angeles County, with Population")

la.census %<>% mutate(sb.per.1000 = sb.count * 1000 / population,
                      sb.per.1000 = ifelse(sb.per.1000 == Inf, NA, sb.per.1000))

la.census %>% arrange(desc(sb.per.1000)) %>% 
  select(population, sb.count, sb.per.1000)
# Filter out the low population tracts
la.census %<>% mutate(sb.per.1000 = ifelse(population < 1000, NA, sb.per.1000))
```




```{r, echo=FALSE, fig.width=4}

LA.map <- la.census %>%
  select(Geo_FIPS, sb.count, sb.per.1000) %>%
  right_join(LA.map, by=c('Geo_FIPS' = 'id'))

ggplot() + geom_polygon(data = LA.map,
                        aes(x = long, y = lat, group = group,
                            fill = rate.poverty)) + 
  labs(title = "Poverty Rate") + theme_bw()+ coord_map()

ggplot() + geom_polygon(data = LA.map,
                        aes(x = long, y = lat, group = group,
                            fill = sb.count)) + 
  labs(title = "Starbuck's Count") + theme_bw()+ coord_map()

ggplot() + geom_polygon(data = LA.map,
                        aes(x = long, y = lat, group = group,
                            fill = sb.per.1000)) + 
  labs(title = "Starbuck's Per 1000 Population") + theme_bw()+ coord_map()
```







## Fitting a Model
Now we need to choose a model for this data. We could try a Poisson model, as
our response variable is a count, but the variance of the data is greater than
the mean, and thus our data is "overdispersed."

```{r}
c(mean(la.census$sb.count), var(la.census$sb.count))
```

To correct for this, we will model the data with a quasi-Poisson family. We are
filtering out the tracts with low population, and then offsetting by population,
so that it is taken into account in the model. 

```{r}
model <- la.census %>% filter(population > 1000) %>%
  glm(sb.count ~ 1, data = ., offset=log(population), 
      family=quasipoisson)
# The average number of Starbucks per 1000 people
exp(model$coefficients)*1000
```

### Starbuck's Counts and Poverty

Now here is the question: Are the distributions of Starbuck's counts, taking
into account population, statistcally different depending on the level of
poverty? 

```{r}
la.census %<>% mutate(poverty = rate.poverty > 40)

modelA <- la.census %>% filter(population > 1000 & poverty) %>%
  glm(sb.count ~ 1, data = ., offset=log(population), 
      family=quasipoisson)
# The average number of Starbucks per 1000 people
exp(modelA$coefficients)*1000


modelB <- la.census %>% filter(population > 1000 & !poverty) %>%
  glm(sb.count ~ 1, data = ., offset=log(population), 
      family=quasipoisson)
# The average number of Starbucks per 1000 people
exp(modelB$coefficients)*1000
```

If we split up the census tracts by whether their povery rate is greater than
40%, we find that the intercept coefficient is fairly different, suggesting there
are different rates in the two groups.

We could try a doing a model with `glm` of `sb.count ~ rate.poverty`, but
our Starbuck's location counts are __not independent__ at all; the number of
Starbuck's near one census tract is highly correlated with the count for a
neighboring census tract.

Essentially our effective sample size is a smaller than the number of census
tracts we observed, because the high correlation means that any given census
tract provides little additional information. 

Instead, we can use a [permutation test](http://en.wikipedia.org/wiki/Resampling_(statistics)#Permutation_tests), which does not make any assumptions
about the distribution the data comes from and will preserve the spatial
relationships between census tracts.

For the quasipoisson model we computed for this data, given the intercept term
$I$, let the average number of Starbuck's locations per 1,000 people is
$\lambda = 1000 \cdot exp(I).$ Let $\lambda_A$ be the $\lambda$ value for the
census tracts that have a poverty rate greater than 40% and $\lambda_B$ be the
value for the rest of the census tracts. Then our hypotheses will be:

* $H_0: \lambda_A - \lambda_B = 0$
* $H_A: \lambda_A - \lambda_B \neq 0$

Our test statistical that we observed from our sample is:

```{r}
T_obs <- exp(modelA$coefficients)*1000 - exp(modelB$coefficients)*1000
T_obs
```

A permutation test involves calculating the test statistic for every 
permutation of the `poverty` variable, to find the null permutation
distribution. This is because the null hypothesis assumes that the `poverty`
variable carries no information about the response variable, and thus should
have no effect on the test statistic. But, we have 2,255 observations, and thus,
would have to calculate test statistics for $2,254!$ permutations! (Try typing
`factorial(2254)` into R, and it will just return `Inf`. That's how big that is.)
So instead of calculating every permutation, we will use Monte Carlo sampling 
to sample a more manageable number of permutations and use the sampling
distribution as an approximation of the full null permutation distribution. 


```{r, echo=FALSE}
count <- la.census %>% filter(poverty) %>% count() %>% .$n
iterations <- 5000
  
# Create a column of shuffle-able poverty bool values
LA.shuffle <- la.census %>%
  filter(population > 1000) %>%
  select(poverty, population, sb.count)

calcCoef <- function (df) {
  # Computes the average number of Starbucks per 1000 people
  model <- glm(sb.count ~ 1, data = df, offset = log(population),
               family = quasipoisson)
  return(exp(model$coefficients)*1000)
}

sampleT <- function (n) {
  shuffle <- LA.shuffle %>% nrow() %>% sample()
  LA.shuffle$poverty <- LA.shuffle$poverty[shuffle]
  
  lambdaA <- LA.shuffle %>% filter(poverty) %>% calcCoef()
  lambdaB <- LA.shuffle %>% filter(!poverty) %>% calcCoef()
  return(lambdaA - lambdaB)
}

Tvalues <- numeric(length=iterations)

Tvalues <- foreach(i=1:iterations, .combine=c) %dopar% sampleT(count)

qplot(x = Tvalues) + 
  geom_vline(xintercept=T_obs, color = 'red') + 
  theme_bw() + 
  labs(title = "Null Distribution of Poverty - Non-Poverty")

```

The probability of getting a value of $T$ as likely or less likely than our 
$T_{obs}$ from the null permutation distribution is 

```{r}
2 * (sum(Tvalues >= T_obs) + 1)/(iterations+1)
```

Thus, we can say with a confidence of 98% there is a statistically significant
difference between the distribution of Starbuck's counts in poorer and wealthier
areas.

We can construct confidence intervals for these values using Bootstrapping, a
method that in similar in principle to what we just did. Bootstrapping
estimates the distribution of a statistic about a set of data by calculating
that statistic on a large number of samples from the data set, each of which
are the same size as the original data but sampled with replacement. From this
approximated distribution, we can then get the 95% confidence intervals by
finding the quantiles for 0.025 and 0.975.

```{r, echo=FALSE}
bootstrap.CI <- function(iterations) {
  # Place to store the bootstrapped samples
  boot.sample.T <- numeric(length = iterations)
  boot.sample.F <- numeric(length = iterations)
  # New subsetted data to sample from
  data.T <- la.census %>% filter(poverty & population > 1000)
  data.F <- la.census %>% filter(!poverty & population > 1000)
  # Define function to do bootstrapping
  bootstrap <- function (data) {
    data %>% sample_n(nrow(data), replace=TRUE) %>%
      calcCoef() %>% return()
  }
  # Perform first bootstrap
  boot.sample.T <- foreach(i=1:iterations, .combine=c) %dopar% bootstrap(data.T)

  # Perform second bootstrap
  boot.sample.F <- foreach(i=1:iterations, .combine=c) %dopar% bootstrap(data.F)

  alpha <- 0.05
  CI <- c(alpha/2, 1 - alpha/2)
  list(quantile(boot.sample.T, CI),
       quantile(boot.sample.F, CI)) %>% return()
}
```

```{r, warning=FALSE}
bootstrap.CI(5000)
```

## Conclusions
I found that there the rate of Starbuck's per 1,000 people in high-poverty
census tracts is higher than the low-poverty rate in Los Angeles County. In
some ways, I find this counter-intuitive; why would Starbuck's locations be
more numerous where more of the population in the area probably cannot afford
to buy a $4.00 lattés all the time?

Either my intuition is wrong, or there is something missing from my model. One
thing I did not account for is the population densities of census tracts. It is
possible that Starbuck's are simply spread out geographically such that areas
with more people will naturally have more Starbuck's per 1,000 people.

It's important to note that these results are not generalizable outside of
Los Angeles County; all the data examined was limited to that area.




